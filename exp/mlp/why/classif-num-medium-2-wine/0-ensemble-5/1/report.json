{
    "function": "bin.ensemble.main",
    "config": {
        "seeds": [
            5,
            6,
            7,
            8,
            9
        ]
    },
    "single_model_function": "bin.ffn.main",
    "data": ":data/classif-num-medium-2-wine",
    "prediction_type": "probs",
    "metrics": {
        "train": {
            "0": {
                "precision": 0.9932960893854749,
                "recall": 0.9758507135016465,
                "f1-score": 0.9844961240310076,
                "support": 911
            },
            "1": {
                "precision": 0.9753363228699552,
                "recall": 0.9931506849315068,
                "f1-score": 0.9841628959276018,
                "support": 876
            },
            "accuracy": 0.9843312814773363,
            "macro avg": {
                "precision": 0.9843162061277151,
                "recall": 0.9845006992165767,
                "f1-score": 0.9843295099793047,
                "support": 1787
            },
            "weighted avg": {
                "precision": 0.9844920852066303,
                "recall": 0.9843312814773363,
                "f1-score": 0.9843327732651522,
                "support": 1787
            },
            "cross-entropy": 0.12748116431822312,
            "roc-auc": 0.9985915422361897,
            "score": 0.9843312814773363
        },
        "val": {
            "0": {
                "precision": 0.8555555555555555,
                "recall": 0.7333333333333333,
                "f1-score": 0.7897435897435897,
                "support": 105
            },
            "1": {
                "precision": 0.8,
                "recall": 0.896,
                "f1-score": 0.8452830188679246,
                "support": 125
            },
            "accuracy": 0.8217391304347826,
            "macro avg": {
                "precision": 0.8277777777777777,
                "recall": 0.8146666666666667,
                "f1-score": 0.8175133043057572,
                "support": 230
            },
            "weighted avg": {
                "precision": 0.8253623188405796,
                "recall": 0.8217391304347826,
                "f1-score": 0.8199280620937716,
                "support": 230
            },
            "cross-entropy": 0.4595411904143653,
            "roc-auc": 0.8728380952380952,
            "score": 0.8217391304347826
        },
        "test": {
            "0": {
                "precision": 0.796812749003984,
                "recall": 0.7662835249042146,
                "f1-score": 0.78125,
                "support": 261
            },
            "1": {
                "precision": 0.7867132867132867,
                "recall": 0.8152173913043478,
                "f1-score": 0.8007117437722421,
                "support": 276
            },
            "accuracy": 0.7914338919925512,
            "macro avg": {
                "precision": 0.7917630178586353,
                "recall": 0.7907504581042812,
                "f1-score": 0.7909808718861211,
                "support": 537
            },
            "weighted avg": {
                "precision": 0.7916219639160278,
                "recall": 0.7914338919925512,
                "f1-score": 0.7912526839499792,
                "support": 537
            },
            "cross-entropy": 0.4806693627119035,
            "roc-auc": 0.8708007107557332,
            "score": 0.7914338919925512
        }
    }
}
